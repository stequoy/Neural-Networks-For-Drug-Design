{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f6a5090",
      "metadata": {
        "id": "6f6a5090"
      },
      "source": [
        "# Simple Impementation of E(n) Equivariant Graph Neural Network for Highest occupied molecular orbital energy\n",
        "\n",
        "Original paper https://arxiv.org/pdf/2102.09844.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bU4ixrOJCg1",
      "metadata": {
        "id": "4bU4ixrOJCg1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb08a10",
      "metadata": {
        "id": "8cb08a10"
      },
      "source": [
        "# Load QM9 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859f981c",
      "metadata": {
        "id": "859f981c"
      },
      "outputs": [],
      "source": [
        "from qm9.data_utils import get_data, BatchGraph\n",
        "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e20004",
      "metadata": {
        "id": "05e20004"
      },
      "source": [
        "# Graph Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0acbcc0",
      "metadata": {
        "id": "d0acbcc0",
        "outputId": "f8fa9dd8-c9e6-4785-e5f7-06ee2ea8ce0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In the batch: num_graphs 96 num_nodes 1759\n",
              "> .h \t\t a tensor of nodes representations \t\tshape 1759 x 15\n",
              "> .x \t\t a tensor of nodes positions  \t\t\tshape 1759 x 3\n",
              "> .edges \t a tensor of edges, a fully connected graph \tshape 31288 x 2\n",
              "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784c0726",
      "metadata": {
        "id": "784c0726"
      },
      "source": [
        "# Define Equivariant Graph Convs  & GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e5e05f",
      "metadata": {
        "id": "76e5e05f"
      },
      "outputs": [],
      "source": [
        "def index_sum(agg_size, source, idx, cuda):\n",
        "    \"\"\"\n",
        "        source is N x hid_dim [float]\n",
        "        idx    is N           [int]\n",
        "\n",
        "        Sums the rows source[.] with the same idx[.];\n",
        "    \"\"\"\n",
        "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
        "    tmp = tmp.cuda() if cuda else tmp\n",
        "    res = torch.index_add(tmp, 0, idx, source)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5d55db",
      "metadata": {
        "id": "4d5d55db"
      },
      "outputs": [],
      "source": [
        "class ConvEGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.cuda = cuda\n",
        "\n",
        "        # computes messages based on hidden representations -> [0, 1]\n",
        "        self.f_e = nn.Sequential(\n",
        "            nn.Linear(in_dim*2+1, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU())\n",
        "\n",
        "        # preducts \"soft\" edges based on messages\n",
        "        self.f_inf = nn.Sequential(\n",
        "            nn.Linear(hid_dim, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        # updates hidden representations -> [0, 1]\n",
        "        self.f_h = nn.Sequential(\n",
        "            nn.Linear(in_dim+hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "\n",
        "    def forward(self, b):\n",
        "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
        "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1)\n",
        "\n",
        "        # compute messages\n",
        "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
        "        m_ij = self.f_e(tmp)\n",
        "\n",
        "        # predict edges\n",
        "        e_ij = self.f_inf(m_ij)\n",
        "\n",
        "        # average e_ij-weighted messages\n",
        "        # m_i is num_nodes x hid_dim\n",
        "        m_i = index_sum(b.h.shape[0], e_ij*m_ij, b.edges[:,0], self.cuda)\n",
        "\n",
        "        # update hidden representations\n",
        "        b.h += self.f_h(torch.hstack([b.h, m_i]))\n",
        "\n",
        "        return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10aad7c4",
      "metadata": {
        "id": "10aad7c4"
      },
      "outputs": [],
      "source": [
        "class NetEGNN(nn.Module):\n",
        "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "\n",
        "        self.emb = nn.Linear(in_dim, hid_dim)\n",
        "\n",
        "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for _ in range(n_layers)]\n",
        "        self.gnn = nn.Sequential(*self.gnn)\n",
        "\n",
        "        self.pre_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "\n",
        "        self.post_mlp = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, out_dim))\n",
        "\n",
        "        if cuda: self.cuda()\n",
        "        self.cuda = cuda\n",
        "\n",
        "    def forward(self, b):\n",
        "        b.h = self.emb(b.h)\n",
        "\n",
        "        b = self.gnn(b)\n",
        "        h_nodes = self.pre_mlp(b.h)\n",
        "\n",
        "        # h_graph is num_graphs x hid_dim\n",
        "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda)\n",
        "\n",
        "        out = self.post_mlp(h_graph)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f4cef6",
      "metadata": {
        "id": "b7f4cef6"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "cuda = True\n",
        "\n",
        "model = NetEGNN(n_layers=7, cuda=cuda)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5d6b1c",
      "metadata": {
        "id": "4e5d6b1c"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3613c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de3613c9",
        "outputId": "a924add2-aadc-4669-e7cb-3a92c13ba5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> start training\n",
            "> epoch 000: "
          ]
        }
      ],
      "source": [
        "print('> start training')\n",
        "\n",
        "tr_ys = train_loader.dataset.data['homo']\n",
        "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
        "\n",
        "if cuda:\n",
        "    me = me.cuda()\n",
        "    mad = mad.cuda()\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True)\n",
        "    start = time.time()\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_val_loss = []\n",
        "    batch_test_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = BatchGraph(batch, cuda, charge_scale)\n",
        "\n",
        "        out = model(batch).reshape(-1)\n",
        "        loss =  F.l1_loss(out,  (batch.y-me)/mad)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
        "\n",
        "        batch_train_loss += [float(loss.data.cpu().numpy())]\n",
        "\n",
        "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
        "\n",
        "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_val_loss += [np.mean(loss)]\n",
        "\n",
        "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
        "\n",
        "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
        "\n",
        "        for batch in test_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_test_loss += [np.mean(loss)]\n",
        "\n",
        "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7825a8f6",
      "metadata": {
        "id": "7825a8f6"
      },
      "outputs": [],
      "source": [
        "# > start training\n",
        "# > epoch 000: train 264.008 val 243.351 test 242.635 (94.3 sec)\n",
        "# > epoch 001: train 211.893 val 211.575 test 210.144 (92.8 sec)\n",
        "# > epoch 002: train 185.362 val 164.960 test 165.087 (93.9 sec)\n",
        "# > epoch 003: train 163.121 val 150.953 test 150.533 (93.2 sec)\n",
        "# ...\n",
        "# > epoch 998: train 0.032 val 30.157 test 30.886 (93.4 sec)\n",
        "# > epoch 999: train 0.032 val 30.157 test 30.886 (93.4 sec)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}